---
title: "test"
output:
  pdf_document: default
  html_document: default
date: "2022-11-06"
---

```{r}
#test4
# df = read.csv("amazon_review_polarity_csv/train.csv", header = FALSE)
```

```{r}
set.seed(1)
N = 100000
N_t = 0.8*N
reviews_text<-readLines("amazon_review_polarity_csv/train.csv", n = N)
reviews_text<-data.frame(reviews_text)
```

```{r}
library(tidyr) 
reviews_text<-separate(data = reviews_text, col = reviews_text, into = c("Sentiment", "SentimentText"), sep = 4)
```

```{r}
# Retaining only alphanumeric values in the sentiment column
reviews_text$Sentiment<-gsub("[^[:alnum:] ]","",reviews_text$Sentiment) 
# Retaining only alphanumeric values in the sentiment text
reviews_text$SentimentText<-gsub("[^[:alnum:] ]"," ",reviews_text$SentimentText) 
# Replacing multiple spaces in the text with single space
reviews_text$SentimentText<-gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", reviews_text$SentimentText, perl=TRUE) 
# Writing the output to a file that can be consumed in other projects
write.table(reviews_text,file = "Sentiment Analysis Dataset.csv",row.names = F,col.names = T,sep=',')
```

```{r}
# reading the first 1000 reviews from the dataset
reviews_text<-readLines('amazon_review_polarity_csv/train.csv', n = N)
# basic EDA to confirm that the data is read correctly 
print(class(reviews_text)) 
print(length(reviews_text)) 
print(head(reviews_text,2)) 
# replacing the positive sentiment value 2 with __label__2
reviews_text<-gsub("\\\"2\\\",","__label__2 ",reviews_text) 
# replacing the negative sentiment value 1 with __label__1
reviews_text<-gsub("\\\"1\\\",","__label__1 ",reviews_text) 
# removing the unnecessary \" characters 
reviews_text<-gsub("\\\""," ",reviews_text) 
# replacing multiple spaces in the text with single space
reviews_text<-gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", reviews_text, perl=TRUE) 
# Basic EDA post the required processing to confirm input is as desired 
print("EDA POST PROCESSING") 
print(class(reviews_text)) 
print(length(reviews_text)) 
print(head(reviews_text,2)) 
# writing the revamped file to the directory so we could use it with 
# fastText sentiment analyzer project
fileConn<-file("Sentiment Analysis Dataset_ft.txt") 
writeLines(reviews_text, fileConn) 
close(fileConn)
```

## BoW approach
```{r}
# including the required libraries 
library(SnowballC) 
library(tm) 
# setting the working directory where the text reviews dataset is located 
# reading the transformed file as a dataframe 
text <- read.table(file='Sentiment Analysis Dataset.csv', sep=',', header = TRUE) 
# checking the dataframe to confirm everything is in tact 
print(dim(text)) 
# View(text)
```

```{r}
# transforming the text into volatile corpus 
train_corp = VCorpus(VectorSource(text$SentimentText)) 
print(train_corp)
```

```{r}
# creating document term matrix 
dtm_train <- DocumentTermMatrix(train_corp, control = list( tolower = TRUE,removeNumbers = TRUE, stopwords = TRUE, removePunctuation = TRUE, stemming = TRUE )) 
# Basic EDA on dtm 
inspect(dtm_train)
```

```{r}
# Removing sparse terms 
dtm_train = removeSparseTerms(dtm_train, 0.99) 
inspect(dtm_train)
```

```{r}
# splitting the train and test DTM 
dtm_train_train <- dtm_train[1:N_t, ] 
dtm_train_test <- dtm_train[(N_t+1):N, ] 
dtm_train_train_labels <- as.factor(as.character(text[1:N_t, ]$Sentiment)) 
dtm_train_test_labels <- as.factor(as.character(text[(N_t+1):N, ]$Sentiment))
```

```{r}
cellconvert<- function(x) { x <- ifelse(x > 0, "Y", "N") }
```

```{r}
# applying the function to rows in training and test datasets 
dtm_train_train <- apply(dtm_train_train, MARGIN = 2,cellconvert) 
dtm_train_test <- apply(dtm_train_test, MARGIN = 2,cellconvert) 
# inspecting the train dtm to confirm all is in tact 
# View(dtm_train_train)
```

### training model
```{r}
# training the naive bayes classifier on the training dtm 
library(e1071) 
nb_senti_classifier=naiveBayes(dtm_train_train,dtm_train_train_labels) 
# printing the summary of the model created 
summary(nb_senti_classifier)
```

```{r}
# making predictions on the test data dtm 
nb_predicts <- predict(nb_senti_classifier, dtm_train_test,type="class") 
```

```{r}
# computing accuracy of the model 
library(rminer) 
print(mmetric(nb_predicts, dtm_train_test_labels, c("ACC")))
```


### pretrained word2vec word embedding
```{r}
# including the required library 
# install.packages("https://cran.r-project.org/src/contrib/Archive/softmaxreg/softmaxreg_1.2.tar.gz",repos = NULL, type = "source")
library(softmaxreg) 
# importing the word2vec pretrained vector into memory 
data(word2vec)
# View(word2vec)
dim(word2vec)
```

```{r}
# function to get word vector for each review 
docVectors = function(x) { wordEmbed(x, word2vec, meanVec = TRUE) } 
text = read.csv(file='Sentiment Analysis Dataset.csv', header = TRUE) 
# applying the docVector function on each of the reviews 
# storing the matrix of word vectors as temp 
temp=t(sapply(text$SentimentText, docVectors)) 
# visualizing the word vectors output 
# View(temp)
dim(temp)
```

```{r}
# splitting the dataset into train and test 
temp_train=temp[1:N_t,] 
temp_test=temp[(N_t+1):N,] 
labels_train=as.factor(as.character(text[1:N_t,]$Sentiment)) 
labels_test=as.factor(as.character(text[(N_t+1):N,]$Sentiment)) 
# including the random forest library 
library(randomForest) 
# training a model using random forest classifier with training dataset 
# observe that we are using 20 trees to create the model 
rf_senti_classifier=randomForest(temp_train, labels_train,ntree=20) 
print(rf_senti_classifier)
```

```{r}
# making predictions on the dataset 
rf_predicts<-predict(rf_senti_classifier, temp_test) 
library(rminer) 
print(mmetric(rf_predicts, labels_test, c("ACC")))
```

### GloVe word embedding
```{r}
# including the required library 
library(text2vec) 
# reading the dataset 
text = read.csv(file='Sentiment Analysis Dataset.csv', header = TRUE) 
# subsetting only the review text so as to create Glove word embedding 
wiki = as.character(text$SentimentText) 
# Create iterator over tokens 
tokens = space_tokenizer(wiki) 
# Create vocabulary. Terms will be unigrams (simple words). 
it = itoken(tokens, progressbar = FALSE) 
vocab = create_vocabulary(it) 
# consider a term in the vocabulary if and only if the term has appeared at least three times in the dataset 
vocab = prune_vocabulary(vocab, term_count_min = 3L) 
# Use the filtered vocabulary
vectorizer = vocab_vectorizer(vocab) 
# use window of 5 for context words and create a term co-occurance matrix 
tcm = create_tcm(it, vectorizer, skip_grams_window = 5L) 
# create the glove embedding for each in the vocab and 
# the dimension of the word embedding should set to 50 
# x_max is the maximum number of co-occurrences to use in the weighting 
# function 
# note that training the word embedding is time consuming - be patient 
glove = GlobalVectors$new(rank = 50, x_max = 100)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01)
```

```{r}
# Glove model learns two sets of word vectors - main and context. 
# both matrices may be added to get the combined word vector 
wv_context = glove$components 
word_vectors = wv_main + t(wv_context) 
# converting the word_vector to a dataframe for visualization 
word_vectors=data.frame(word_vectors) 
# the word for each embedding is set as row name by default 
# using the tibble library rownames_to_column function, the rownames is copied as first column of the dataframe 
# we also name the first column of the dataframe as words 
library(tibble) 
word_vectors=rownames_to_column(word_vectors, var = "words") 
# View(word_vectors)
```

```{r}
library(softmaxreg) 
docVectors = function(x) { wordEmbed(x, word_vectors, meanVec = TRUE) } 
# applying the function docVectors function on the entire reviews dataset 
# this will result in word embedding representation of the entire reviews 
# dataset 
temp=t(sapply(text$SentimentText, docVectors)) 
# View(temp)
```

```{r}
# splitting the dataset into train and test portions 
temp_train=temp[1:N_t,] 
temp_test=temp[(N_t+1):N,] 
labels_train=as.factor(as.character(text[1:N_t,]$Sentiment)) 
labels_test=as.factor(as.character(text[(N_t+1):N,]$Sentiment)) 
# using randomforest to build a model on train data library(randomForest)
rf_senti_classifier=randomForest(temp_train, labels_train,ntree=20) 
print(rf_senti_classifier)
```

```{r}
# predicting labels using the randomforest model created 
rf_predicts<-predict(rf_senti_classifier, temp_test) 
# estimating the accuracy from the predictions 
library(rminer) 
print(mmetric(rf_predicts, labels_test, c("ACC")))
```

### fastText word embedding
```{r}
# loading the required libary 
library(fastTextR) 
# reading the input reviews file 
# recollect that fastText needs the file in a specific format and we created one compatiable file in 
# "Understanding the Amazon Reviews Dataset" section of this chapter
text = readLines("Sentiment Analysis Dataset_ft.txt") 
# Viewing the text vector for conformation 
# View(text)
```

```{r}
# dividing the reviews into training and test 
temp_train=text[1:N_t]
temp_test=text[(N_t+1):N] 
# Viewing the train datasets for confirmation 
# View(temp_train)
```

```{r}
# creating txt file for train and test dataset 
# the fasttext function expects files to be passed for training and testing 
fileConn<-file("train.ft.txt") 
writeLines(temp_train, fileConn) 
close(fileConn) 
fileConn<-file("test.ft.txt") 
writeLines(temp_test, fileConn) 
close(fileConn) 
# creating a test file with no labels 
# recollect the original test dataset has labels in it 
# as the dataset is just a subset obtained from full dataset 
temp_test_nolabel<- gsub("__label__1", "", temp_test, perl=TRUE) 
temp_test_nolabel<- gsub("__label__2", "", temp_test_nolabel, perl=TRUE)
# View(temp_test_nolabel)
```

```{r}
fileConn<-file("test_nolabel.ft.txt") 
writeLines(temp_test_nolabel, fileConn) 
close(fileConn) 
# training a supervised classification model with training dataset file 
model<-ft_train("train.ft.txt", method = "supervised", control = ft_control(nthreads = 3L, seed = 1)) 
# Obtain all the words from a previously trained model= 
words<-ft_words(model) 
# viewing the words for confirmation. These are the set of words present in our training data 
# View(words)
```

```{r}
# Obtain word vectors from a previously trained model. 
word_vec<-ft_word_vectors(model, words) 
# Viewing the word vectors for each word in our training dataset
# observe that the word embedding dimension is 5 
# View(word_vec)
```

```{r}
# predicting the labels for the reviews in the no labels test dataset 
# getting the predictions into a dataframe so as to compute performance measurement
ft_preds<-ft_predict(model, newdata = temp_test_nolabel) 
# reading the test file to extract the actual labels 
reviewstestfile<- readLines("test.ft.txt") 
# extracting just the labels frm each line 
library(stringi) 
actlabels<-stri_extract_first(reviewstestfile, regex="\\w+") 
# converting the actual labels and predicted labels into factors 
actlabels<-as.factor(as.character(actlabels)) 
ft_preds<-as.factor(as.character(ft_preds$label)) 
# getting the estimate of the accuracy 
library(rminer) 
print(mmetric(actlabels, ft_preds, c("ACC")))
```


# Drug Data
```{r}
set.seed(1)
N_Drug = 146942
reviews_text_Drug<-readLines("Drug Train.csv", n = N_Drug)
reviews_text_Drug<-data.frame(reviews_text_Drug)
```

```{r}
library(tidyr) 
reviews_text_Drug<-separate(data = reviews_text_Drug, col = reviews_text_Drug, into = c("Sentiment", "SentimentText"), sep = 4)
reviews_text_Drug<-reviews_text_Drug[-1,]
N_Drug = N_Drug - 1
```

```{r}
# Retaining only alphanumeric values in the sentiment column
reviews_text_Drug$Sentiment<-gsub("[^[:alnum:] ]","",reviews_text_Drug$Sentiment) 
# Retaining only alphanumeric values in the sentiment text
reviews_text_Drug$SentimentText<-gsub("[^[:alnum:] ]"," ",reviews_text_Drug$SentimentText) 
# Replacing multiple spaces in the text with single space
reviews_text_Drug$SentimentText<-gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", reviews_text_Drug$SentimentText, perl=TRUE)
```


```{r}
# Balance our data
minlabel<-names(which(table(reviews_text_Drug$Sentiment)==min(table(reviews_text_Drug$Sentiment))))
maxlabel<-names(which(table(reviews_text_Drug$Sentiment)==max(table(reviews_text_Drug$Sentiment))))

n_maxlabel<-min(table(reviews_text_Drug$Sentiment))
minlabelid<-c(1:N_Drug)[reviews_text_Drug$Sentiment==minlabel]
maxlabelid<-sample(c(1:N_Drug)[reviews_text_Drug$Sentiment==maxlabel],n_maxlabel)
balanceid<-sample(c(minlabelid,maxlabelid))
reviews_text_Drug<-reviews_text_Drug[balanceid,]

N_Drug = nrow(reviews_text_Drug)
N_train_Drug = round(0.8*N_Drug)
```


```{r}
# Writing the output to a file that can be consumed in other projects
write.table(reviews_text_Drug,file = "Sentiment Analysis Dataset_Drug.csv",row.names = F,col.names = T,sep=',')
```

```{r}
# reading the first 1000 reviews from the dataset
reviews_text_Drug<-readLines("Drug Train.csv", n = 146942)
reviews_text_Drug<-reviews_text_Drug[-1]
reviews_text_Drug<-reviews_text_Drug[balanceid]
# basic EDA to confirm that the data is read correctly 
print(class(reviews_text_Drug)) 
print(length(reviews_text_Drug)) 
print(head(reviews_text_Drug,2)) 
# replacing the positive sentiment value 2 with __label__2
reviews_text_Drug<-gsub("\\\"2\\\",","__label__2 ",reviews_text_Drug) 
# replacing the negative sentiment value 1 with __label__1
reviews_text_Drug<-gsub("\\\"1\\\",","__label__1 ",reviews_text_Drug) 
# removing the unnecessary \" characters 
reviews_text_Drug<-gsub("\\\""," ",reviews_text_Drug) 
# replacing multiple spaces in the text with single space
reviews_text_Drug<-gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", reviews_text_Drug, perl=TRUE) 
# Basic EDA post the required processing to confirm input is as desired 
print("EDA POST PROCESSING") 
print(class(reviews_text_Drug)) 
print(length(reviews_text_Drug)) 
print(head(reviews_text_Drug,2)) 
# writing the revamped file to the directory so we could use it with 
# fastText sentiment analyzer project
fileConn<-file("Sentiment Analysis Dataset_ft_Drug.txt") 
writeLines(reviews_text_Drug, fileConn) 
close(fileConn)
```

## BoW approach
```{r}
# including the required libraries 
library(SnowballC) 
library(tm) 
# setting the working directory where the text reviews dataset is located 
# reading the transformed file as a dataframe 
text_Drug <- read.table(file='Sentiment Analysis Dataset_Drug.csv', sep=',', header = TRUE) 
# checking the dataframe to confirm everything is in tact 
print(dim(text_Drug)) 
# View(text)
```

```{r}
# transforming the text into volatile corpus 
train_corp_Drug = VCorpus(VectorSource(text_Drug$SentimentText)) 
print(train_corp_Drug)
```

```{r}
# creating document term matrix 
dtm_train_Drug <- DocumentTermMatrix(train_corp_Drug, control = list( tolower = TRUE,removeNumbers = TRUE, stopwords = TRUE, removePunctuation = TRUE, stemming = TRUE )) 
# Basic EDA on dtm 
inspect(dtm_train_Drug)
```

```{r}
# Removing sparse terms 
dtm_train_Drug = removeSparseTerms(dtm_train_Drug, 0.99) 
inspect(dtm_train_Drug)
```

```{r}
# splitting the train and test DTM 
dtm_train_train_Drug <- dtm_train_Drug[1:N_train_Drug, ] 
dtm_train_test_Drug <- dtm_train_Drug[(N_train_Drug+1):N_Drug, ] 
dim(dtm_train_Drug)
dtm_train_train_Drug_labels <- as.factor(as.character(text_Drug[1:N_train_Drug, ]$Sentiment)) 
dtm_train_test_Drug_labels <- as.factor(as.character(text_Drug[(N_train_Drug+1):N_Drug, ]$Sentiment))
```

```{r}
cellconvert<- function(x) { x <- ifelse(x > 0, "Y", "N") }
```

```{r}
# applying the function to rows in training and test datasets 
dtm_train_train_Drug <- apply(dtm_train_train_Drug, MARGIN = 2,cellconvert) 
dtm_train_test_Drug <- apply(dtm_train_test_Drug, MARGIN = 2,cellconvert) 
# inspecting the train dtm to confirm all is in tact 
# View(dtm_train_train)
```

### training model
```{r}
# training the naive bayes classifier on the training dtm 
library(e1071) 
nb_senti_classifier_Drug=naiveBayes(dtm_train_train_Drug,dtm_train_train_Drug_labels) 
# printing the summary of the model created 
summary(nb_senti_classifier_Drug)
```

```{r}
# making predictions on the test data dtm 
nb_predicts_Drug <- predict(nb_senti_classifier_Drug, dtm_train_test_Drug,type="class") 
# printing the predictions from the model 
print(nb_predicts_Drug)
```

```{r}
# computing accuracy of the model 
library(rminer) 
print(mmetric(nb_predicts_Drug, dtm_train_test_Drug_labels, c("ACC")))
```


### pretrained word2vec word embedding
```{r}
# including the required library 
# install.packages("https://cran.r-project.org/src/contrib/Archive/softmaxreg/softmaxreg_1.2.tar.gz",repos = NULL, type = "source")
library(softmaxreg) 
# importing the word2vec pretrained vector into memory 
data(word2vec)
# View(word2vec)
dim(word2vec)
```

```{r}
# function to get word vector for each review 
docVectors = function(x) { wordEmbed(x, word2vec, meanVec = TRUE) } 
text_Drug = read.csv(file='Sentiment Analysis Dataset_Drug.csv', header = TRUE) 
# applying the docVector function on each of the reviews 
# storing the matrix of word vectors as temp 
temp_Drug=t(sapply(text_Drug$SentimentText, docVectors)) 
# visualizing the word vectors output 
# View(temp)
dim(temp_Drug)
```

```{r}
# splitting the dataset into train and test 
temp_train_Drug=temp_Drug[1:N_train_Drug,] 
temp_test_Drug=temp_Drug[(N_train_Drug+1):N_Drug,] 
labels_train_Drug=as.factor(as.character(text_Drug[1:N_train_Drug,]$Sentiment)) 
labels_test_Drug=as.factor(as.character(text_Drug[(N_train_Drug+1):N_Drug,]$Sentiment)) 
# including the random forest library 
library(randomForest) 
# training a model using random forest classifier with training dataset 
# observe that we are using 20 trees to create the model 
rf_senti_classifier_Drug=randomForest(temp_train_Drug, labels_train_Drug,ntree=20) 
print(rf_senti_classifier_Drug)
```

```{r}
# making predictions on the dataset 
rf_predicts_Drug<-predict(rf_senti_classifier_Drug, temp_test_Drug) 
library(rminer) 
print(mmetric(rf_predicts_Drug, labels_test_Drug, c("ACC")))
```

### GloVe word embedding
```{r}
# including the required library 
library(text2vec) 
# reading the dataset 
text_Drug = read.csv(file='Sentiment Analysis Dataset_Drug.csv', header = TRUE) 
# subsetting only the review text so as to create Glove word embedding 
wiki_Drug = as.character(text_Drug$SentimentText) 
# Create iterator over tokens 
tokens_Drug = space_tokenizer(wiki_Drug) 
# Create vocabulary. Terms will be unigrams (simple words). 
it_Drug = itoken(tokens_Drug, progressbar = FALSE) 
vocab_Drug = create_vocabulary(it_Drug) 
# consider a term in the vocabulary if and only if the term has appeared at least three times in the dataset 
vocab_Drug = prune_vocabulary(vocab_Drug, term_count_min = 3L) 
# Use the filtered vocabulary
vectorizer_Drug = vocab_vectorizer(vocab_Drug) 
# use window of 5 for context words and create a term co-occurance matrix 
tcm_Drug = create_tcm(it_Drug, vectorizer_Drug, skip_grams_window = 5L) 
# create the glove embedding for each in the vocab and 
# the dimension of the word embedding should set to 50 
# x_max is the maximum number of co-occurrences to use in the weighting 
# function 
# note that training the word embedding is time consuming - be patient 
glove = GlobalVectors$new(rank = 50, x_max = 100)
wv_main_Drug = glove$fit_transform(tcm_Drug, n_iter = 10, convergence_tol = 0.01)
```

```{r}
# Glove model learns two sets of word vectors - main and context. 
# both matrices may be added to get the combined word vector 
wv_context = glove$components 
word_vectors_Drug = wv_main_Drug + t(wv_context) 
# converting the word_vector to a dataframe for visualization 
word_vectors_Drug=data.frame(word_vectors_Drug) 
# the word for each embedding is set as row name by default 
# using the tibble library rownames_to_column function, the rownames is copied as first column of the dataframe 
# we also name the first column of the dataframe as words 
library(tibble) 
word_vectors_Drug=rownames_to_column(word_vectors_Drug, var = "words") 
# View(word_vectors)
```

```{r}
library(softmaxreg) 
docVectors_Drug = function(x) { wordEmbed(x, word_vectors_Drug, meanVec = TRUE) } 
# applying the function docVectors function on the entire reviews dataset 
# this will result in word embedding representation of the entire reviews 
# dataset 
temp_Drug=t(sapply(text_Drug$SentimentText, docVectors_Drug)) 
# View(temp)
```

```{r}
# splitting the dataset into train and test portions 
temp_train_Drug=temp_Drug[1:N_train_Drug,] 
temp_test_Drug=temp_Drug[(N_train_Drug+1):N_Drug,] 
labels_train_Drug=as.factor(as.character(text_Drug[1:N_train_Drug,]$Sentiment)) 
labels_test_Drug=as.factor(as.character(text_Drug[(N_train_Drug+1):N_Drug,]$Sentiment)) 
# using randomforest to build a model on train data library(randomForest)
rf_senti_classifier_Drug=randomForest(temp_train_Drug, labels_train_Drug,ntree=20) 
print(rf_senti_classifier_Drug)
```

```{r}
# predicting labels using the randomforest model created 
rf_predicts_Drug<-predict(rf_senti_classifier_Drug, temp_test_Drug) 
# estimating the accuracy from the predictions 
library(rminer) 
print(mmetric(rf_predicts_Drug, labels_test_Drug, c("ACC")))
```

### fastText word embedding
```{r}
# loading the required libary 
library(fastTextR) 
# reading the input reviews file 
# recollect that fastText needs the file in a specific format and we created one compatiable file in 
# "Understanding the Amazon Reviews Dataset" section of this chapter
text_Drug = readLines("Sentiment Analysis Dataset_ft_Drug.txt") 
# Viewing the text vector for conformation 
# View(text)
```

```{r}
# dividing the reviews into training and test 
temp_train_Drug=text_Drug[1:N_train_Drug]
temp_test_Drug=text_Drug[(N_train_Drug+1):N_Drug] 
# Viewing the train datasets for confirmation 
# View(temp_train)
```

```{r}
# creating txt file for train and test dataset 
# the fasttext function expects files to be passed for training and testing 
fileConn<-file("train_Drug.ft.txt") 
writeLines(temp_train_Drug, fileConn) 
close(fileConn) 
fileConn<-file("test_Drug.ft.txt") 
writeLines(temp_test_Drug, fileConn) 
close(fileConn) 
# creating a test file with no labels 
# recollect the original test dataset has labels in it 
# as the dataset is just a subset obtained from full dataset 
temp_test_Drug_nolabel<- gsub("__label__1", "", temp_test_Drug, perl=TRUE) 
temp_test_Drug_nolabel<- gsub("__label__2", "", temp_test_Drug_nolabel, perl=TRUE)
# View(temp_test_nolabel)
```

```{r}
fileConn<-file("test_Drug_nolabel.ft.txt") 
writeLines(temp_test_Drug_nolabel, fileConn) 
close(fileConn) 
# training a supervised classification model with training dataset file 
model_Drug<-ft_train("train_Drug.ft.txt", method = "supervised", control = ft_control(nthreads = 3L, seed = 1)) 
# Obtain all the words from a previously trained model= 
words_Drug<-ft_words(model_Drug) 
# viewing the words for confirmation. These are the set of words present in our training data 
# View(words)
```

```{r}
# Obtain word vectors from a previously trained model. 
word_vec_Drug<-ft_word_vectors(model_Drug, words_Drug) 
# Viewing the word vectors for each word in our training dataset
# observe that the word embedding dimension is 5 
# View(word_vec)
```

```{r}
# predicting the labels for the reviews in the no labels test dataset 
# getting the predictions into a dataframe so as to compute performance measurement
ft_preds_Drug<-ft_predict(model_Drug, newdata = temp_test_Drug_nolabel) 
# reading the test file to extract the actual labels 
reviewstestfile_Drug<- readLines("test_Drug.ft.txt") 
# extracting just the labels frm each line 
library(stringi) 
actlabels_Drug<-stri_extract_first(reviewstestfile_Drug, regex="\\w+") 
# converting the actual labels and predicted labels into factors 
actlabels_Drug<-as.factor(as.character(actlabels_Drug)) 
ft_preds_Drug<-as.factor(as.character(ft_preds_Drug$label)) 
# getting the estimate of the accuracy 
library(rminer) 
print(mmetric(actlabels_Drug, ft_preds_Drug, c("ACC")))
```

















