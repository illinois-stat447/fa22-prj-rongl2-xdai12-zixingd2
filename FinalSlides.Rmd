---
title: "Sentiment Analysis for Amazon Review Dataset & Drug Dataset"
author: "Rong Li, Xiongjie Dai, Zixing Deng"
output: 
  beamer_presentation: default
  ioslides_presentation: 
    css: style.css
    transition: 0
    widescreen: yes
date: "2022-12-08"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Model 4: FastText word Embedding

- fastText: breaking words into n-grams (subwords), and generating word embeddings by taking the sum of those subwords

  - e.g. 2-grams for word *help* will be "he, el, lp"

- `fastTextR` package:

  - `ft_control`: set hyperparameter for fastText

  - `ft_train`: train the model

  - `ft_predict`: predict values based on the trained model
  
  - `ft_test`: evaluate the model

- Final results:

  - *Amazon Review* dataset: 86.48%
  
  - *Drug Review* dataset: 78.96%


## Conclusion & Discussion

- Results in glance:

                      Bow         word2vec      GloVe         FastText 
----------------- ----------- --------------- ----------    ------------
 Amazon Review      81.19%        62.88%        71.99%         86.48%
  Drug Review       74.77%        71.02%        74.83%         78.69%

- Possible improvements for `word2vec`:

  - create specific word embeddings for the dataset to minimize the effect of pre-trained embeddings
  
- Future directions:

  - try more classification models such as XGBoost and AdaBoost
  
  - split the dataset into train, validation, test subsets


# Thank you! 
