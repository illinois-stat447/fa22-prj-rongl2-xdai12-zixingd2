---
title: "Final Report"
output: pdf_document
date: "`r Sys.Date()`"
bibliography: References.bib
nocite: '@*'
---

```{=tex}
\newpage 
\tableofcontents 
\newpage
```
# Introduction

Machine learning techniques have become increasingly popular and relevant to solve text and sentiment-related problems in recent years. It has boosted performance on several tasks and significantly reduced the necessity for human efforts. For this project, we focused on text classification, especially sentiment analysis, on two datasets, *Amazon Review* and *Drug Review*. While the *Amazon Review* dataset is popular and was being used for many research papers and projects, we decided to replicate the code of four classic machine learning models from existing literature and adapted them to a newer but less popular dataset we found in the UCI Machine Learning Repository. Our goal for the project is to compare classifiers including \verb|BoW|, \verb|Word2Vec|, \verb|GloVe|, \verb|fastText| for two different datasets. 

# Dataset Overview & Preporcess
For the *Amazon Review* dataset, we used the same dataset mentioned by @AmazonData, which was obtained from the link provided by @rCode^[Google Drive Link: https://drive.google.com/drive/u/0/folders/0Bz8a_Dbh9Qhbfll6bVpmNUtUcFdjYmF2SEpmZUZUcVNiMUw1TWN6RDV3a0JHT3kxLVhVR2M]. The dataset contains about 1,800,000 training samples and 200,000 testing samples with three attributes, which are classification labels (1 for negative reviews and 2 for positive reviews), the title of each review text, and the review text body. Due to the limit of computer computation ability, we pulled out the first 100,000 data samples and split 80% of the data into the training set and 20% of the data into the testing set. Below are the head rows from the data:
``` {r amazon head}
```
For the Drug dataset, we downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29). The dataset has 215,063 samples with 6 attributes, including \verb|drugName|, \verb|condition|, \verb|review|, \verb|rating| (1 to 10), \verb|date|, \verb|usefulCount|. Similar to *Amazon Review* dataset, we split the whole dataset into training (80%) and testing (20%) datasets. In order to replicate the code, we categorized the dataset into two labels, where \verb|rating|s range from [need to be filled] are categorized as [1(negative) or 2(positive)] and the rest is categorized as []. Moreover, we only retained three columns (i.e., \verb|review|, \verb|rating|, [\verb|condition|]) to make the dataset have the same format as *Amazon Review*. Below are the head rows of the preprocessed data:
``` {r drug head}
```



# Model 1: Bag of Words with Naive Bayes algorithm
Bag of Words (BoW) method is widely used in NLP and computer vision fields. It takes the occurrence of each word in the text regardless of grammar and makes it into “bags” to characterize the text. To implement BoW method for our dataset, *Amazon Review* and Drug dataset, we first use \verb|VCorpus| function and \verb|DocumentTermMatrix| function in the \verb|tm| package to convert text into a matrix. By adjusting the built-in parameter in the  \verb|DocumentTermMatrix|, we do not have to worry about cleaning the dataset with stop words and punctuation. In order to make the model more precise, we removed words that do not occur in 99% of the documents by using \verb|removeSparseTerms| function. After finishing the process of BoW conversion, we followed @rCode and used the Naive Bayes sentiment classifier to perform predictions. Utilizing the \verb|nb_sent_classifier| in the \verb|e1071| package, we obtained the prediction results with [accuracy] for *Amazon Review* data and [accuracy] for Drug data.  


# Model 4: FastText word embedding 
As an extension of \verb|word2vec|, \verb|fastTextR| package is used to reach more concise predictions for the analysis. Created and open-sourced by Facebook in 2016 [@fastText], fastText is a more powerful tool to classify text and learn word vector representation by breaking words into character n-grams. Before training the model, we convert the label in the dataset from “\\\1\\\” into “__label__1” in order to meet the format of the fastText algorithm. We also cleaned all multiple spaces in the text with a single space. Thereupon, we used \verb|ft_train| function to train the model and \verb|ft_control| to tune the hyper-parameter for our two datasets. Our best accuracy for the fastText model is **86.48** for the *Amazon Review Dataset* and **78.69%** for the Drug dataset. 



# Conclusion & Discussion
In conclusion, comparing all of our models after fine-tuning, the fastText model performs best on the *Amazon Review Dataset* with **86.48%** of accuracy and the Drug dataset with **78.69%** of accuracy. Since words passed by the fastText model are represented as the sum of each word’s bag of character n-grams, fastText is much more efficient to deal with large corpus and compute word embeddings for words unseen from the training set [@whyFT]. With such features, fastText can cope with typos and different word tenses accordingly without treating them as different words. For example, “helped” and “help” are two same words but only different from tenses. However, models other than fastText may treat them as two different words and assign the wrong labels. Therefore, using fastText can significantly boost performance. 

From the entire scope, the word2vec model performs with relatively low accuracy for both models (62.88% for *Amazon Review* and 71.02% for Drug). One possible reason will be the existing embedding in the \verb|word2vec|, which treats all unknown words as zero vectors. Thus, we may want to try to create and train word embedding on our own using CBOW and Continuous Skip-Gram in the future to see if any improvements will be made.

For future investigation, we can try to use BERT to better process the dataset and attempt more classification algorithms, such as XGBoost and AdaBoost, to classify the sentence embeddings. Moreover, as we only split our data into train and test datasets with 80-20, we may want to split the test set further into 10% of the validation dataset and 10% of the testing dataset so that we can select more fitting hyperparameters for the model and realize higher accuracy [@validSet]. 



# References

<!-- ```{r setup, include=FALSE} -->

<!-- knitr::opts_chunk$set(echo = TRUE) -->

<!-- ``` -->

<!-- ## R Markdown -->

<!-- This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

<!-- ```{r cars} -->

<!-- summary(cars) -->

<!-- ``` -->

<!-- ## Including Plots -->

<!-- You can also embed plots, for example: -->

<!-- ```{r pressure, echo=FALSE} -->

<!-- plot(pressure) -->

<!-- ``` -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->
